{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music genre classification with naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have helper functions for the required functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the probability density for a gaussian function\n",
    "def gaussian(x, mu, sig):\n",
    "    return (1/(sig*np.sqrt(2*math.pi))) * np.exp(-np.power(x-mu, 2) / (2*np.power(sig, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the prior probability for all labels, \n",
    "# modified code form tutorial 3\n",
    "def calc_prior(data):\n",
    "    prior_p = {} # dictionary of prior probabilities\n",
    "\n",
    "    labels = data.values[:, -1]\n",
    "    n = len(labels)\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    for i in range(len(unique_labels)):\n",
    "        prior_p[unique_labels[i]] = (counts[i] / n)\n",
    "\n",
    "    return prior_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the mean and standard deviation for each attribute given its label\n",
    "def calc_distributions(data):\n",
    "    avg_vals = data.groupby(['label']).mean()\n",
    "    std_vals = data.groupby(['label']).std()\n",
    "\n",
    "    return (avg_vals, std_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the posterior probabilities for all items in the data set\n",
    "# modified from tutorial 3\n",
    "def calc_posterior(data, prior_p, distributions):\n",
    "    epsilon = 1e-8 # Value used instead of 0 when finding logs\n",
    "    posterior_probs = []\n",
    "    features_list = data.columns \n",
    "\n",
    "    for _, instance in data.iterrows():\n",
    "        post_p = {}\n",
    "\n",
    "        for label in prior_p:\n",
    "            post_p[label] = np.log(prior_p[label])\n",
    "            \n",
    "            for feature in features_list:\n",
    "                x = instance[feature]\n",
    "                mu = distributions[0][feature][label]\n",
    "                sig = distributions[1][feature][label]\n",
    "\n",
    "                prob = gaussian(x, mu, sig)\n",
    "                if(prob == 0): prob = epsilon\n",
    "\n",
    "                post_p[label] += np.log(prob)\n",
    "        \n",
    "        posterior_probs.append(post_p)\n",
    "\n",
    "    return posterior_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tp_tn_fp_fn(actual, predicted):\n",
    "    tp = 0 # true positive\n",
    "    tn = 0 # true negative\n",
    "    fp = 0 # false positive\n",
    "    fn = 0 # false negative\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == 'classical' and predicted[i] == 'classical':\n",
    "            tp += 1\n",
    "        elif actual[i] == 'pop' and predicted[i] == 'classical':\n",
    "            fp += 1\n",
    "        elif actual[i] == 'pop' and predicted[i] == 'pop':\n",
    "            tn += 1\n",
    "        elif actual[i] == 'classical' and predicted[i] == 'pop':\n",
    "            fn += 1\n",
    "\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepares the data by reading it from a file and converting it into a useful format for training and testing\n",
    "def preprocess(file):\n",
    "    out = pd.read_csv(file)\n",
    "\n",
    "    return out.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should calculate prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "def train(data):\n",
    "    # Calculate prior probabilities\n",
    "    prior_p = calc_prior(data)\n",
    "\n",
    "    # Calculate distributions\n",
    "    distributions = calc_distributions(data)\n",
    "\n",
    "    return prior_p, distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset, \n",
    "# modified from tutorial 3\n",
    "\n",
    "def predict(data, prior_p, distributions):\n",
    "    # calculate posterior probabilities\n",
    "    posterior_p = calc_posterior(data, prior_p, distributions)\n",
    "    \n",
    "    # find argmax for each instance\n",
    "    argmax_labels = []\n",
    "    for post_p in posterior_p:\n",
    "        max_prob = float('-inf')\n",
    "        max_label = None\n",
    "        for label in post_p:\n",
    "            if label in prior_p:\n",
    "                prob = post_p[label]\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_label = label\n",
    "        if max_label is not None:\n",
    "            argmax_labels.append(max_label)\n",
    "\n",
    "    return argmax_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your modelâ€™s class outputs to ground\n",
    "# truth labels\n",
    "def evaluate(predictions, data):\n",
    "    tp, tn, fp, fn = find_tp_tn_fp_fn(data.values[:, -1].tolist(), predictions)\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    \n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Pop vs. classical music classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Compute and report the accuracy, precision, and recall of your model (treat \"classical\" as the \"positive\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9767441860465116\n",
      "Precision is: 0.9523809523809523\n",
      "Recall is: 1.0\n"
     ]
    }
   ],
   "source": [
    "# First, we preprocess the data\n",
    "train_df = preprocess(\"music_data/pop_vs_classical_train.csv\")\n",
    "test_df = preprocess(\"music_data/pop_vs_classical_test.csv\")\n",
    "\n",
    "# Next, we train the model\n",
    "prior_p, distr = train(train_df)\n",
    "\n",
    "# Now, we predict labels for the test_data\n",
    "predictions = predict(test_df.iloc[:,:-1], prior_p, distr)\n",
    "\n",
    "# Now find our evaluation metrics\n",
    "accuracy, precision, recall = evaluate(predictions, test_df)\n",
    "print(f\"Accuracy is: {accuracy}\\nPrecision is: {precision}\\nRecall is: {recall}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "For each of the features X below, plot the probability density functions P(X|Class = pop) and P(X|Class = classical). If you had to classify pop vs. classical music using just one of these three features, which feature would you use and why? Refer to your plots to support your answer.\n",
    "- spectral centroid mean\n",
    "- harmony mean\n",
    "- tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pdf(distr, feature, title, filename, axis):\n",
    "    plt.plot(axis, norm.pdf(axis, distr[0][feature]['classical'], distr[1][feature]['classical']), color='b')\n",
    "    plt.plot(axis, norm.pdf(axis, distr[0][feature]['pop'], distr[1][feature]['pop']), color='r')\n",
    "    plt.legend([\"classical\", \"pop\"], loc=\"upper right\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct pdf for spectral_centroid_mean\n",
    "make_pdf(distr, 'spectral_centroid_mean', \n",
    "         \"PDFs of Spectral Centroid Mean for Each Label\", \n",
    "         'scm_pdf.png', np.arange(400, 4600, 0.1))\n",
    "\n",
    "# Construct pdf for harmony_mean\n",
    "make_pdf(distr, 'harmony_mean',\n",
    "         \"PDFs of Harmony Mean for Each Label\",\n",
    "         'hm_pdf.png', np.arange(-0.004, 0.012, 0.000001))\n",
    "\n",
    "# Construct pdf for tempo\n",
    "make_pdf(distr, 'tempo', \n",
    "         \"PDFs of Tempo for Each Label\", \n",
    "         'tempo_pdf.png', np.arange(65, 235, 0.001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. 10-way music genre classification\n",
    "\n",
    "#### NOTE: you may develop codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts values using 0R\n",
    "def zero_r_predictions(train, test):\n",
    "    out = []\n",
    "\n",
    "    prediction = train['label'].mode()\n",
    "    for i in range(len(test)):\n",
    "        out.append(prediction)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts values using one-attribute baseline\n",
    "def one_attr_predictions(train_df, test_df):\n",
    "    # Make a data frame to store attributes for one-attribute naive bayes and \n",
    "    # their error rates\n",
    "    accuracy_df = pd.DataFrame(columns = ['accuracy', 'predictions'])\n",
    "\n",
    "    # List of all attributes, besides label\n",
    "    feature_list = train_df.iloc[: , :-1].columns\n",
    "\n",
    "    # Iterate over each feature and construct Naive Bayes\n",
    "    for feature in feature_list:\n",
    "        # Train using one attribute\n",
    "        prior_p, distr = train(train_df[[feature, 'label']])\n",
    "\n",
    "        # Predict using one attribute model\n",
    "        predictions = predict(test_df[[feature]], prior_p, distr)\n",
    "\n",
    "        # Save accuracy rate\n",
    "        accuracy_df.loc[feature] = [metrics.accuracy_score(test_df['label'].tolist(), predictions), predictions]\n",
    "\n",
    "    # Find feature with highest accuracy\n",
    "    best_feature = accuracy_df['accuracy'].idxmax()\n",
    "    print(f\"Feature used for one-attribute naive Bayes is {best_feature}\\n\")\n",
    "\n",
    "    return accuracy_df.at[best_feature, 'predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we preprocess the data\n",
    "train_df = preprocess(\"music_data/gztan_train.csv\")\n",
    "test_df = preprocess(\"music_data/gztan_train.csv\")\n",
    "\n",
    "# Next, we train the model\n",
    "prior_p, distr = train(train_df)\n",
    "\n",
    "# Now, we predict labels for the test_data\n",
    "predictions = predict(test_df.iloc[:,:-1], prior_p, distr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Compare the performance of the full model to a 0R baseline and a one-attribute baseline. The one-attribute baseline should be the best possible naive Bayes model which uses only a prior and a single attribute. In your write-up, explain how you implemented the 0R and one-attribute baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature used for one-attribute naive Bayes is chroma_stft_mean\n",
      "\n",
      "Overall accuracy score of Naive Bayes is: 0.5875\n",
      "Overall accuracy score of Zero R Baseline is: 0.1075\n",
      "Overall accuracy score of One Attribute Baseline is: 0.28125\n"
     ]
    }
   ],
   "source": [
    "labels = test_df['label'].unique()\n",
    "\n",
    "# Find actual labels of test values\n",
    "true_labels = test_df.values[:, -1].tolist() \n",
    "\n",
    "# To prepare for cross-model performance over categories\n",
    "model_predictions = [predictions, \n",
    "                    zero_r_predictions(train_df, test_df), \n",
    "                    one_attr_predictions(train_df, test_df)]\n",
    "model_names = [\"Naive Bayes\", \"Zero R Baseline\", \"One Attribute Baseline\"]\n",
    "cf_filenames = [\"nb_cf.png\", \"zero_r_cf.png\", \"one_attr_cf.png\"]\n",
    "\n",
    "for i in range(3):\n",
    "    # Evaluating accuracy of model\n",
    "    accuracy = metrics.accuracy_score(true_labels, model_predictions[i])\n",
    "    print(f\"Overall accuracy score of {model_names[i]} is: {accuracy}\")\n",
    "\n",
    "    # Evaluating model based on performance over categories\n",
    "    conf_mat = metrics.confusion_matrix(true_labels, model_predictions[i])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    conf_mat_display = metrics.ConfusionMatrixDisplay(conf_mat, \n",
    "                    display_labels = labels)\n",
    "    conf_mat_display.plot()\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    plt.title(f\"{model_names[i]} Confusion Matrix\")\n",
    "    plt.savefig(cf_filenames[i])\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
